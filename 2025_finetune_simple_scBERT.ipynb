{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'src.performer_pytorch.performer_pytorch' from '/data/lyx/scCHiP/scATAC/LDA/PBMC_10k/src/performer_pytorch/performer_pytorch.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import scanpy as sc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import torch\n",
    "from scipy.sparse import lil_matrix, csr_matrix\n",
    "import importlib\n",
    "import src.utils_BERT\n",
    "from src.performer_pytorch.performer_pytorch import PerformerLM\n",
    "importlib.reload(src.utils_BERT)\n",
    "importlib.reload(src.performer_pytorch.performer_pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/data/lyx/scCHiP/scATAC/LDA/PBMC_10k/processed_data'\n",
    "work_dir = '/data/lyx/scCHiP/scATAC/LDA/PBMC_10k'\n",
    "sc_count_file = os.path.join(data_dir,\"10k_PBMC_Multiome_filtered_gene_count.h5ad\")\n",
    "sc_anno_file = os.path.join(data_dir,\"MainCelltype.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data = pd.read_table(sc_anno_file, header=None,index_col=0)\n",
    "meta_data.columns = [\"Celltype\"]\n",
    "cell_type = list(set(meta_data.Celltype))\n",
    "ntopics_list = list(range(len(cell_type), 3*len(cell_type)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sc.read_h5ad(sc_count_file)\n",
    "gene_names = pd.read_table(os.path.join(work_dir,\"src/data_BERT/gene2vec_16906_names.txt\"),sep=\"\\t\",header=None)[0].to_list()\n",
    "data = data[:,data.var_names.isin(gene_names)].copy()\n",
    "indices = [index for index, element in enumerate(gene_names) if element in data.var_names] \n",
    "\n",
    "sc.pp.normalize_total(data, target_sum=1e4)\n",
    "sc.pp.log1p(data)\n",
    "\n",
    "data_csr = np.zeros((data.shape[0], len(gene_names)), dtype=np.float32) \n",
    "data_csr[:, indices]=np.array(data.X.todense())\n",
    "data_csr = csr_matrix(data_csr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names, label = np.unique(np.array(meta_data['Celltype']), return_inverse=True)  \n",
    "# Convert strings categorical to integrate categorical, and label_names[label] can be restored\n",
    "#store the label dict and label for prediction\n",
    "# with open('label_names', 'wb') as fp:\n",
    "#     pkl.dump(label_names, fp)\n",
    "# with open('label', 'wb') as fp:\n",
    "#     pkl.dump(label, fp)\n",
    "\n",
    "class_num = np.unique(label, return_counts=True)[1].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight = torch.tensor([(1 - (x / sum(class_num))) ** 2 for x in class_num])\n",
    "label = torch.from_numpy(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, ShuffleSplit, StratifiedShuffleSplit, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, precision_recall_fscore_support, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "from torch.optim.lr_scheduler import StepLR, CosineAnnealingWarmRestarts, CyclicLR\n",
    "from cosine_annealing_warmup import CosineAnnealingWarmupRestarts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "# import argparse\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "import random\n",
    "from functools import reduce\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit, StratifiedShuffleSplit, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, precision_recall_fscore_support, classification_report\n",
    "\n",
    "class SCDataset(Dataset):\n",
    "    def __init__(self, data, label,CLASS,device):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        self.CLASS = CLASS\n",
    "        self.device = device\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        rand_start = random.randint(0, self.data.shape[0]-1)\n",
    "        full_seq = self.data[rand_start].toarray()[0]\n",
    "        full_seq[full_seq > (self.CLASS - 2)] = self.CLASS - 2\n",
    "        full_seq = torch.from_numpy(full_seq).long()\n",
    "        full_seq = torch.cat((full_seq, torch.tensor([0]))).to(self.device)\n",
    "        seq_label = self.label[rand_start]\n",
    "        return full_seq, seq_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "class MinimalIdentity(torch.nn.Module):\n",
    "    def __init__(self,out_dim = 10,SEQ_LEN=16907):\n",
    "        super(MinimalIdentity, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 1, (1, 200))\n",
    "        self.act = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(in_features=SEQ_LEN, out_features=out_dim, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x[:,None,:,:]\n",
    "        x = self.conv1(x)\n",
    "        x = self.act(x)\n",
    "        x = x.view(x.shape[0],-1)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "class MinimalClassifier(nn.Module):\n",
    "    def __init__(self, embed_dim=200, num_classes=6):\n",
    "        super().__init__()\n",
    "        self.pool = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool1d(1),  # [batch,200,1]\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        self.classifier = nn.Linear(200, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.classifier(self.pool(x.permute(0,2,1)))\n",
    "\n",
    "    \n",
    "class MinimalIdentity_v2(torch.nn.Module):\n",
    "    def __init__(self,out_dim = 10,SEQ_LEN=16907,h_dim=512,dropout=0.0):\n",
    "        super(MinimalIdentity_v2, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 1, (1, 200))\n",
    "        self.act = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(in_features=SEQ_LEN, out_features=h_dim, bias=True)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(in_features=h_dim, out_features=out_dim, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x[:,None,:,:]\n",
    "        x = self.conv1(x)\n",
    "        x = self.act(x)\n",
    "        x = x.view(x.shape[0],-1)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "    \n",
    "class GeneChannelAttention(nn.Module):\n",
    "    \"\"\"基因通道注意力（移除位置相关操作）\"\"\"\n",
    "    def __init__(self, in_dim):\n",
    "        super().__init__()\n",
    "        self.channel_scale = nn.Sequential(\n",
    "            nn.Linear(in_dim, in_dim//4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_dim//4, in_dim),\n",
    "            nn.Sigmoid()  # 添加Sigmoid确保权重在[0,1]范围\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"输入形状: [batch, channels, num_genes]\"\"\"\n",
    "        # 全局平均 → 通道权重\n",
    "        gene_weights = self.channel_scale(x.mean(dim=-1))  # [batch, channels]\n",
    "        # 加权聚合所有基因特征\n",
    "        return (x * gene_weights.unsqueeze(-1)).sum(dim=-1)  # [batch, channels]\n",
    "\n",
    "class BioClassifier_v2(nn.Module):\n",
    "    def __init__(self, num_genes=16907, embed_dim=200, num_classes=6):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 特征聚合模块\n",
    "        self.feature_aggregator = nn.Sequential(\n",
    "            GeneChannelAttention(200),    # 通道注意力加权\n",
    "            nn.BatchNorm1d(200),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        \n",
    "        # 分类器\n",
    "        self.classifier = nn.Sequential(\n",
    "            # nn.Linear(200, 512),\n",
    "            # nn.GELU(),\n",
    "            nn.Linear(200, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x形状: [batch, num_genes, embed_dim]\n",
    "        x = x.permute(0, 2, 1)         # [24,200,16907] (channels_first)\n",
    "        x = self.feature_aggregator(x) # [24,200]\n",
    "        return self.classifier(x)      # [24,6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# first train #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1\n",
    "BATCH_SIZE = 24\n",
    "CLASS = 5+2 #Number of bins.'+2\n",
    "SEQ_LEN = len(gene_names)+1#gene_num\", type=int, default=16906\n",
    "POS_EMBED_USING = True #'Using Gene2vec encoding or not.'\n",
    "LEARNING_RATE = 1e-4\n",
    "EPOCHS = 20\n",
    "GRADIENT_ACCUMULATION = 60\n",
    "VALIDATE_EVERY =1\n",
    "PATIENCE = 10\n",
    "UNASSIGN_THRES = 0.0\n",
    "path = \"/data/lyx/scCHiP/scATAC/LDA/PBMC_10k/src/data_BERT/panglao_pretrain.pth\"\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "POS_EMBED_USING =True\n",
    "model_name = \"2025_finetune_simple_PBMC_MainCelltype_scBert\"\n",
    "ckpt_dir = os.path.join(work_dir,\"scBert_model\",\"scBert_PBMC_10k/\")\n",
    "# world_size = torch.distributed.get_world_size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16906"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gene_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# tree train #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 3\n",
    "BATCH_SIZE = 24\n",
    "CLASS = 5+2 #Number of bins.'+2\n",
    "SEQ_LEN = len(gene_names)+1#gene_num\", type=int, default=16906\n",
    "POS_EMBED_USING = True #'Using Gene2vec encoding or not.'\n",
    "LEARNING_RATE = 1e-4\n",
    "EPOCHS = 20\n",
    "GRADIENT_ACCUMULATION = 60\n",
    "VALIDATE_EVERY =1\n",
    "PATIENCE = 10\n",
    "UNASSIGN_THRES = 0.0\n",
    "path = \"/data/lyx/scCHiP/scATAC/LDA/PBMC_10k/scBert_model/scBert_PBMC_10k/2025_finetune_simple_PBMC_MainCelltype_scBert_v3_best.pth\"\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "POS_EMBED_USING =True\n",
    "model_name = \"2025_finetune_simple_PBMC_MainCelltype_scBert_v4\"\n",
    "ckpt_dir = os.path.join(work_dir,\"scBert_model\",\"scBert_PBMC_10k/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## test ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试代码\n",
    "test_input = torch.randn(24, 16907, 200)  # 模拟真实输入\n",
    "model = BioClassifier()\n",
    "\n",
    "# 逐步检查维度变化\n",
    "x_proj = model.proj(test_input)          # 应得到[24,16907,256]\n",
    "x_perm = x_proj.permute(0, 2, 1)         # → [24,256,16907]\n",
    "conv_out = model.feature_extractor[0](x_perm)  # → [24,256,16907]\n",
    "print(conv_out.shape)  # 应保持通道数256\n",
    "\n",
    "# 完整前向传播验证\n",
    "output = model(test_input)\n",
    "print(output.shape)  # 预期[24,6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## run ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = []\n",
    "f1 = []\n",
    "f1w = []\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "pred_list = pd.Series(['un'] * data_csr.shape[0])\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=SEED)\n",
    "for index_train, index_val in sss.split(data_csr, label):\n",
    "    data_train, label_train = data_csr[index_train], label[index_train]\n",
    "    data_val, label_val = data_csr[index_val], label[index_val]\n",
    "    train_dataset = SCDataset(data_train, label_train,CLASS,device)\n",
    "    val_dataset = SCDataset(data_val, label_val,CLASS,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = src.utils_BERT.SimpleSampler(train_dataset)\n",
    "val_sampler = src.utils_BERT.SimpleSampler(val_dataset)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, sampler=val_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "16907 % 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/lyx/scCHiP/scATAC/LDA/PBMC_10k/src/performer_pytorch/performer_pytorch.py:175: UserWarning: torch.qr is deprecated in favor of torch.linalg.qr and will be removed in a future PyTorch release.\n",
      "The boolean parameter 'some' has been replaced with a string parameter 'mode'.\n",
      "Q, R = torch.qr(A, some)\n",
      "should be replaced with\n",
      "Q, R = torch.linalg.qr(A, 'reduced' if some else 'complete') (Triggered internally at ../aten/src/ATen/native/BatchLinearAlgebra.cpp:2416.)\n",
      "  q, r = torch.qr(unstructured_block.cpu(), some = True)\n"
     ]
    }
   ],
   "source": [
    "model = PerformerLM(\n",
    "    num_tokens = CLASS,\n",
    "    dim = 200,\n",
    "    depth = 6,\n",
    "    max_seq_len = SEQ_LEN,\n",
    "    heads = 10,\n",
    "    gene_weight_file='./src/data_BERT/gene2vec_16906.npy',\n",
    "    local_attn_heads = 0,\n",
    "    g2v_position_emb = POS_EMBED_USING\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to_out = MinimalIdentity(out_dim=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2010758/1144557707.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(path,map_location=device)\n"
     ]
    }
   ],
   "source": [
    "ckpt = torch.load(path,map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(ckpt['model_state_dict'])\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.norm.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model.performer.net.layers[-2].parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to_out = MinimalIdentity_v2(out_dim = 6,SEQ_LEN=16907,h_dim=512,dropout=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2010758/2988717159.py:5: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=USE_AMP)\n"
     ]
    }
   ],
   "source": [
    "USE_AMP = True  # 启用混合精度\n",
    "MAX_GRAD_NORM = 1.0  # 梯度裁剪阈值\n",
    "\n",
    "# 初始化混合精度训练\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=USE_AMP)\n",
    "\n",
    "# 改进的参数分组策略\n",
    "param_groups = [\n",
    "    {'params': model.performer.parameters(), 'lr': 1e-5, 'weight_decay': 0.001},\n",
    "    {'params': model.to_out.parameters(), 'lr': 1e-3, 'weight_decay': 0.01}\n",
    "]\n",
    "\n",
    "# 动态调整的优化器配置\n",
    "optimizer = torch.optim.AdamW(param_groups)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, \n",
    "    max_lr=[group['lr'] for group in param_groups],  # 分组学习率峰值\n",
    "    total_steps=1000,\n",
    "    pct_start=0.2\n",
    ")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(weight=None).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=5, min_delta=0.001):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_f1 = -float('inf')\n",
    "        self.counter = 0\n",
    "\n",
    "    def __call__(self, val_f1):\n",
    "        if val_f1 > self.best_f1 + self.min_delta:\n",
    "            self.best_f1 = val_f1\n",
    "            self.counter = 0\n",
    "            return False\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            return self.counter >= self.patience\n",
    "\n",
    "early_stopper = EarlyStopper(patience=PATIENCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "# 训练主循环（完整改进版）\n",
    "def train_model(model, optimizer, scheduler, train_loader, val_loader, \n",
    "                loss_fn, device, EPOCHS, GRADIENT_ACCUMULATION, \n",
    "                MAX_GRAD_NORM, VALIDATE_EVERY, UNASSIGN_THRES, \n",
    "                early_stopper, model_name, ckpt_dir):\n",
    "    # 初始化记录器\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "    val_epoch_indices = []\n",
    "    lr_history = []\n",
    "    \n",
    "    # 显式确保模型在目标设备\n",
    "    model = model.to(device)\n",
    "    print(f\"[Init] 模型已加载到设备: {next(model.parameters()).device}\")\n",
    "    \n",
    "    # 梯度缩放器\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=USE_AMP)\n",
    "    \n",
    "    # 获取训练集总样本量（用于精确损失计算）\n",
    "    total_train_samples = len(train_loader.dataset)\n",
    "    \n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        train_loader.sampler.set_epoch(epoch)\n",
    "        model.train()\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # ================== 训练阶段 ==================\n",
    "        for batch_idx, (data, labels) in enumerate(train_loader, 1):\n",
    "            # 确保数据与模型在同一设备\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            assert data.device == labels.device == device  # 设备一致性检查\n",
    "            \n",
    "            # ---- 混合精度前向 ----\n",
    "            with torch.autocast(device_type='cuda', dtype=torch.float16, enabled=USE_AMP):\n",
    "                logits = model(data)\n",
    "                loss = loss_fn(logits, labels) / GRADIENT_ACCUMULATION\n",
    "                \n",
    "            # ---- 异常损失检测 ----\n",
    "            if torch.isnan(loss).any() or torch.isinf(loss).any():\n",
    "                print(f\"[Epoch {epoch} Batch {batch_idx}] 检测到异常损失值: {loss.item():.4f}，跳过该batch\")\n",
    "                optimizer.zero_grad()\n",
    "                continue\n",
    "                \n",
    "            # ---- 梯度累积 ----\n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            # ---- 统计量计算 ----\n",
    "            with torch.no_grad():\n",
    "                preds = logits.detach().argmax(dim=-1)\n",
    "                correct_predictions += (preds == labels).sum().item()\n",
    "                total_samples += labels.size(0)\n",
    "                # 按样本比例计算损失（考虑梯度累积）\n",
    "                running_loss += loss.item() * (data.size(0) / total_train_samples)\n",
    "                \n",
    "            # ---- 参数更新 ----\n",
    "            if batch_idx % GRADIENT_ACCUMULATION == 0 or batch_idx == len(train_loader):\n",
    "                # 梯度裁剪与检查\n",
    "                scaler.unscale_(optimizer)\n",
    "                grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n",
    "                \n",
    "                # 梯度异常处理\n",
    "                if torch.isnan(grad_norm) or torch.isinf(grad_norm):\n",
    "                    print(f\"[Epoch {epoch} Batch {batch_idx}] 检测到异常梯度范数: {grad_norm:.4f}，跳过更新\")\n",
    "                    optimizer.zero_grad()\n",
    "                    scaler.update()  # 必须更新scaler状态\n",
    "                    continue\n",
    "                    \n",
    "                # 优化器步进\n",
    "                try:\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                except RuntimeError as e:\n",
    "                    print(f\"[Epoch {epoch} Batch {batch_idx}] 优化器步进失败: {str(e)}\")\n",
    "                    optimizer.zero_grad()\n",
    "                    continue\n",
    "                    \n",
    "                optimizer.zero_grad()\n",
    "        \n",
    "        # ================== 训练后处理 ==================\n",
    "        # 计算epoch指标\n",
    "        epoch_loss = running_loss * GRADIENT_ACCUMULATION  # 修正累积损失\n",
    "        epoch_acc = 100 * correct_predictions / total_samples\n",
    "        train_loss_history.append(epoch_loss)\n",
    "        lr_history.append(optimizer.param_groups[0]['lr'])\n",
    "        \n",
    "        print(f\"Epoch {epoch} | Train Loss: {epoch_loss:.4f} | Acc: {epoch_acc:.2f}%\")\n",
    "        \n",
    "        # ================== 验证阶段 ==================\n",
    "        if epoch % VALIDATE_EVERY == 0:\n",
    "            model.eval()\n",
    "            val_preds = []\n",
    "            val_labels = []\n",
    "            val_loss = 0.0\n",
    "            \n",
    "            with torch.no_grad(), torch.autocast(device_type='cuda', dtype=torch.float16, enabled=USE_AMP):\n",
    "                for data_v, labels_v in val_loader:\n",
    "                    data_v, labels_v = data_v.to(device), labels_v.to(device)\n",
    "                    logits = model(data_v)\n",
    "                    \n",
    "                    # 损失计算\n",
    "                    val_loss += loss_fn(logits, labels_v).item()\n",
    "                    \n",
    "                    # 预测处理\n",
    "                    probs = torch.softmax(logits, dim=-1)\n",
    "                    preds = probs.argmax(dim=-1)\n",
    "                    mask = probs.max(dim=-1).values >= UNASSIGN_THRES\n",
    "                    val_preds.append(preds[mask].cpu())\n",
    "                    val_labels.append(labels_v[mask].cpu())\n",
    "                    \n",
    "            # 合并结果\n",
    "            val_loss = val_loss / len(val_loader)\n",
    "            val_preds = torch.cat(val_preds) if len(val_preds) > 0 else torch.tensor([])\n",
    "            val_labels = torch.cat(val_labels) if len(val_labels) > 0 else torch.tensor([])\n",
    "            \n",
    "            # 空样本处理\n",
    "            if len(val_labels) == 0:\n",
    "                print(f\"[Epoch {epoch}] 警告：验证集无有效预测样本，跳过指标计算\")\n",
    "                val_f1 = 0.0\n",
    "                val_acc = 0.0\n",
    "            else:\n",
    "                val_f1 = f1_score(val_labels, val_preds, average='macro')\n",
    "                val_acc = accuracy_score(val_labels, val_preds)\n",
    "                print(f\"Epoch {epoch} | Val Loss: {val_loss:.4f} | F1: {val_f1:.4f} | Acc: {val_acc:.2f}%\")\n",
    "                \n",
    "                # 早停判断与模型保存\n",
    "                if val_f1 > early_stopper.best_f1:\n",
    "                    src.utils_BERT.save_best_ckpt(epoch, model, optimizer, scheduler, \n",
    "                                                val_loss, model_name, ckpt_dir)\n",
    "                \n",
    "                if early_stopper(val_f1):\n",
    "                    print(f\"Early stopping at epoch {epoch}\")\n",
    "                    break\n",
    "            \n",
    "            val_loss_history.append(val_loss)\n",
    "            val_epoch_indices.append(epoch)\n",
    "            \n",
    "            # 学习率调度（示例：ReduceLROnPlateau）\n",
    "            if len(val_labels) > 0:\n",
    "                scheduler.step(val_loss)  # 根据验证损失调整\n",
    "            else:\n",
    "                scheduler.step()  # 无验证指标时使用默认调整\n",
    "        \n",
    "        # ================== 资源管理 ==================\n",
    "        # 每5个epoch清理一次显存\n",
    "        if epoch % 5 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "            print(f\"[Memory] 已清理显存，当前使用量: {torch.cuda.memory_allocated(device)/1e9:.2f} GB\")\n",
    "    \n",
    "    return {\n",
    "        \"train_loss\": train_loss_history,\n",
    "        \"val_loss\": val_loss_history,\n",
    "        \"val_epochs\": val_epoch_indices,\n",
    "        \"lr_history\": lr_history\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Init] 模型已加载到设备: cuda:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2010758/1415801169.py:21: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=USE_AMP)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 5.9255 | Acc: 17.72%\n",
      "Epoch 1 | Val Loss: 5.5467 | F1: 0.0269 | Acc: 0.31%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lyx/.conda/envs/STED/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:232: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Train Loss: 5.3046 | Acc: 32.56%\n",
      "Epoch 2 | Val Loss: 4.9285 | F1: 0.1088 | Acc: 0.37%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lyx/.conda/envs/STED/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:232: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Train Loss: 4.6485 | Acc: 39.71%\n",
      "Epoch 3 | Val Loss: 4.2659 | F1: 0.1080 | Acc: 0.42%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lyx/.conda/envs/STED/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:232: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Train Loss: 4.1078 | Acc: 43.15%\n",
      "Epoch 4 | Val Loss: 3.9116 | F1: 0.1072 | Acc: 0.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lyx/.conda/envs/STED/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:232: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Train Loss: 3.6740 | Acc: 44.40%\n",
      "Epoch 5 | Val Loss: 3.3112 | F1: 0.1031 | Acc: 0.45%\n",
      "[Memory] 已清理显存，当前使用量: 0.18 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lyx/.conda/envs/STED/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:232: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 | Train Loss: 3.0593 | Acc: 45.31%\n",
      "Epoch 6 | Val Loss: 2.7151 | F1: 0.1050 | Acc: 0.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lyx/.conda/envs/STED/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:232: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 | Train Loss: 2.3850 | Acc: 44.32%\n",
      "Epoch 7 | Val Loss: 2.1057 | F1: 0.1012 | Acc: 0.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lyx/.conda/envs/STED/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:232: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 | Train Loss: 1.9690 | Acc: 44.27%\n",
      "Epoch 8 | Val Loss: 1.8254 | F1: 0.1039 | Acc: 0.45%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lyx/.conda/envs/STED/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:232: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 | Train Loss: 1.7002 | Acc: 44.79%\n",
      "Epoch 9 | Val Loss: 1.6192 | F1: 0.0993 | Acc: 0.42%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lyx/.conda/envs/STED/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:232: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | Train Loss: 1.5383 | Acc: 44.84%\n",
      "Epoch 10 | Val Loss: 1.5165 | F1: 0.1413 | Acc: 0.46%\n",
      "[Memory] 已清理显存，当前使用量: 0.18 GB\n",
      "CPU times: user 2h 9min 59s, sys: 13.8 s, total: 2h 10min 13s\n",
      "Wall time: 2h 12min 26s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lyx/.conda/envs/STED/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:232: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 调用训练函数\n",
    "training_stats = train_model(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    loss_fn=loss_fn,\n",
    "    device=device,\n",
    "    EPOCHS=EPOCHS,\n",
    "    GRADIENT_ACCUMULATION=GRADIENT_ACCUMULATION,\n",
    "    MAX_GRAD_NORM=MAX_GRAD_NORM,\n",
    "    VALIDATE_EVERY=VALIDATE_EVERY,\n",
    "    UNASSIGN_THRES=UNASSIGN_THRES,\n",
    "    early_stopper=early_stopper,\n",
    "    model_name=model_name,\n",
    "    ckpt_dir=ckpt_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最终验证损失: 1.303528324250252\n"
     ]
    }
   ],
   "source": [
    "# 输出训练结果\n",
    "print(\"最终验证损失:\", training_stats[\"val_loss\"][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_loss': [1.288267878155803,\n",
       "  1.288598245695821,\n",
       "  1.306687069889516,\n",
       "  1.2957995911584594,\n",
       "  1.315560081370334,\n",
       "  1.313788087495373,\n",
       "  1.3372783117582736,\n",
       "  1.3026923176528968,\n",
       "  1.313859963851606,\n",
       "  1.3171471798779317],\n",
       " 'val_loss': [1.3215542863453589,\n",
       "  1.3019985897887139,\n",
       "  1.3039210330574744,\n",
       "  1.310948115683371,\n",
       "  1.3551416704731603,\n",
       "  1.329684595907888,\n",
       "  1.3016742991824304,\n",
       "  1.2968803324526357,\n",
       "  1.2975076653303639,\n",
       "  1.303528324250252],\n",
       " 'val_epochs': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
       " 'lr_history': [3.9999999999999956e-07,\n",
       "  4.010446218278138e-07,\n",
       "  4.010139360665647e-07,\n",
       "  4.0101693239045346e-07,\n",
       "  4.010279224240047e-07,\n",
       "  4.010983927816881e-07,\n",
       "  4.0105751408840324e-07,\n",
       "  4.0101343106229606e-07,\n",
       "  4.0100598029073404e-07,\n",
       "  4.010069537268468e-07]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "src.utils_BERT.save_best_ckpt(\"16\", model, optimizer, scheduler, \n",
    "                              0, '2025_finetune_PBMC_MainCelltype_scBert_last', ckpt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "optimizer = Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = CosineAnnealingWarmupRestarts(\n",
    "    optimizer,\n",
    "    first_cycle_steps=15,\n",
    "    cycle_mult=2,\n",
    "    max_lr=LEARNING_RATE,\n",
    "    min_lr=1e-6,\n",
    "    warmup_steps=5,\n",
    "    gamma=0.9\n",
    ")\n",
    "loss_fn = nn.CrossEntropyLoss(weight=None).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigger_times = 0\n",
    "max_acc = 0.0\n",
    "for i in range(1, EPOCHS+1):\n",
    "    train_loader.sampler.set_epoch(i)\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    cum_acc = 0.0\n",
    "    for index, (data, labels) in enumerate(train_loader):\n",
    "        index += 1\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        if index % GRADIENT_ACCUMULATION != 0:\n",
    "            \n",
    "            logits = model(data)\n",
    "            loss = loss_fn(logits, labels)\n",
    "            loss.backward()\n",
    "            \n",
    "        if index % GRADIENT_ACCUMULATION == 0:      \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), int(1e6))\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        running_loss += loss.item()\n",
    "        softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "        final = softmax(logits)\n",
    "        final = final.argmax(dim=-1)\n",
    "        \n",
    "        #pred_num = labels.size(0)\n",
    "        #correct_num = torch.eq(final, labels).sum(dim=-1)\n",
    "        #cum_acc += torch.true_divide(correct_num, pred_num).mean().item()\n",
    "        cum_acc += torch.eq(final, labels).sum().item() / labels.size(0)\n",
    "        \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = 100 * cum_acc / len(train_loader)\n",
    "    print(f'    ==  Epoch: {i} | Training Loss: {epoch_loss:.6f} | Accuracy: {epoch_acc:4f}%  ==')\n",
    "    \n",
    "    scheduler.step()\n",
    "\n",
    "    if i % VALIDATE_EVERY == 0:\n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        predictions = []\n",
    "        truths = []\n",
    "        with torch.no_grad():\n",
    "            for index, (data_v, labels_v) in enumerate(val_loader):\n",
    "        \n",
    "                data_v, labels_v = data_v.to(device), labels_v.to(device)\n",
    "            \n",
    "                logits = model(data_v)\n",
    "                loss = loss_fn(logits, labels_v)\n",
    "                running_loss += loss.item()\n",
    "                \n",
    "                softmax = nn.Softmax(dim=-1)\n",
    "                final_prob = softmax(logits)\n",
    "                final = final_prob.argmax(dim=-1)\n",
    "                final[np.amax(np.array(final_prob.cpu()), axis=-1) < UNASSIGN_THRES] = -1\n",
    "                \n",
    "                predictions.append(final.cpu())\n",
    "                truths.append(labels_v.cpu())\n",
    "            \n",
    "            predictions = torch.cat(predictions)\n",
    "            truths = torch.cat(truths)\n",
    "            no_drop = predictions != -1\n",
    "            predictions = predictions[no_drop]\n",
    "            truths = truths[no_drop]\n",
    "            \n",
    "            cur_acc = accuracy_score(truths, predictions)\n",
    "            f1 = f1_score(truths, predictions, average='macro')\n",
    "            val_loss = running_loss / len(val_loader)\n",
    "\n",
    "            print(f'Epoch: {i} | Validation Loss: {val_loss:.6f} | F1 Score: {f1:.6f}')\n",
    "            print(confusion_matrix(truths, predictions))\n",
    "            print(classification_report(truths,predictions, target_names=label_names.tolist(), digits=4))\n",
    "            \n",
    "            if cur_acc > max_acc:\n",
    "                max_acc = cur_acc\n",
    "                trigger_times = 0\n",
    "                src.utils_BERT.save_best_ckpt(i, model, optimizer, scheduler, val_loss, model_name, ckpt_dir)\n",
    "            else:\n",
    "                trigger_times += 1\n",
    "                if trigger_times > PATIENCE:\n",
    "                    break\n",
    "    del predictions, truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ==  Epoch: 20 | Training Loss: 0.549712 | Accuracy: 78.317356%  ==\n",
      "Epoch: 20 | Validation Loss: 0.565491 | F1 Score: 0.415135\n"
     ]
    }
   ],
   "source": [
    "print(f'    ==  Epoch: {i} | Training Loss: {epoch_loss:.6f} | Accuracy: {epoch_acc:4f}%  ==')\n",
    "print(f'Epoch: {i} | Validation Loss: {val_loss:.6f} | F1 Score: {f1:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "src.utils_BERT.save_best_ckpt(i, model, optimizer, scheduler, val_loss, '2025_PBMC_MainCelltype_scBert_v1_last', ckpt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SCDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m data_train, label_train \u001b[38;5;241m=\u001b[39m data_csr[index_train], label[index_train]\n\u001b[1;32m     10\u001b[0m data_val, label_val \u001b[38;5;241m=\u001b[39m data_csr[index_val], label[index_val]\n\u001b[0;32m---> 11\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mSCDataset\u001b[49m(data_train, label_train,CLASS,device)\n\u001b[1;32m     12\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m SCDataset(data_val, label_val,CLASS,device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SCDataset' is not defined"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "f1 = []\n",
    "f1w = []\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "pred_list = pd.Series(['un'] * data_csr.shape[0])\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=SEED)\n",
    "for index_train, index_val in sss.split(data_csr, label):\n",
    "    data_train, label_train = data_csr[index_train], label[index_train]\n",
    "    data_val, label_val = data_csr[index_val], label[index_val]\n",
    "    train_dataset = SCDataset(data_train, label_train,CLASS,device)\n",
    "    val_dataset = SCDataset(data_val, label_val,CLASS,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = src.utils_BERT.SimpleSampler(train_dataset)\n",
    "val_sampler = src.utils_BERT.SimpleSampler(val_dataset)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, sampler=val_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/lyx/scCHiP/scATAC/LDA/PBMC_10k/src/performer_pytorch/performer_pytorch.py:175: UserWarning: torch.qr is deprecated in favor of torch.linalg.qr and will be removed in a future PyTorch release.\n",
      "The boolean parameter 'some' has been replaced with a string parameter 'mode'.\n",
      "Q, R = torch.qr(A, some)\n",
      "should be replaced with\n",
      "Q, R = torch.linalg.qr(A, 'reduced' if some else 'complete') (Triggered internally at ../aten/src/ATen/native/BatchLinearAlgebra.cpp:2416.)\n",
      "  q, r = torch.qr(unstructured_block.cpu(), some = True)\n"
     ]
    }
   ],
   "source": [
    "model = PerformerLM(\n",
    "    num_tokens = CLASS,\n",
    "    dim = 200,\n",
    "    depth = 6,\n",
    "    max_seq_len = SEQ_LEN,\n",
    "    heads = 10,\n",
    "    gene_weight_file='./src/data_BERT/gene2vec_16906.npy',\n",
    "    local_attn_heads = 0,\n",
    "    g2v_position_emb = POS_EMBED_USING\n",
    ")\n",
    "model.to_out = Identity(dropout=0., h_dim=128, out_dim=label_names.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1413683/2976891991.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(path,map_location=device)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "ckpt = torch.load(path,map_location=device)\n",
    "model.load_state_dict(ckpt['model_state_dict'])\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.norm.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model.performer.net.layers[-2].parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['epoch', 'model_state_dict', 'optimizer_state_dict', 'scheduler_state_dict', 'losses'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "optimizer = Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = CosineAnnealingWarmupRestarts(\n",
    "    optimizer,\n",
    "    first_cycle_steps=15,\n",
    "    cycle_mult=2,\n",
    "    max_lr=LEARNING_RATE,\n",
    "    min_lr=1e-6,\n",
    "    warmup_steps=5,\n",
    "    gamma=0.9\n",
    ")\n",
    "loss_fn = nn.CrossEntropyLoss(weight=None).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigger_times = 0\n",
    "max_acc = 0.0\n",
    "for i in range(1, EPOCHS+1):\n",
    "    train_loader.sampler.set_epoch(i)\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    cum_acc = 0.0\n",
    "    for index, (data, labels) in enumerate(train_loader):\n",
    "        index += 1\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        if index % GRADIENT_ACCUMULATION != 0:\n",
    "            \n",
    "            logits = model(data)\n",
    "            loss = loss_fn(logits, labels)\n",
    "            loss.backward()\n",
    "            \n",
    "        if index % GRADIENT_ACCUMULATION == 0:      \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), int(1e6))\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        running_loss += loss.item()\n",
    "        softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "        final = softmax(logits)\n",
    "        final = final.argmax(dim=-1)\n",
    "        \n",
    "        #pred_num = labels.size(0)\n",
    "        #correct_num = torch.eq(final, labels).sum(dim=-1)\n",
    "        #cum_acc += torch.true_divide(correct_num, pred_num).mean().item()\n",
    "        cum_acc += torch.eq(final, labels).sum().item() / labels.size(0)\n",
    "        \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = 100 * cum_acc / len(train_loader)\n",
    "    print(f'    ==  Epoch: {i} | Training Loss: {epoch_loss:.6f} | Accuracy: {epoch_acc:4f}%  ==')\n",
    "    \n",
    "    scheduler.step()\n",
    "\n",
    "    if i % VALIDATE_EVERY == 0:\n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        predictions = []\n",
    "        truths = []\n",
    "        with torch.no_grad():\n",
    "            for index, (data_v, labels_v) in enumerate(val_loader):\n",
    "        \n",
    "                data_v, labels_v = data_v.to(device), labels_v.to(device)\n",
    "            \n",
    "                logits = model(data_v)\n",
    "                loss = loss_fn(logits, labels_v)\n",
    "                running_loss += loss.item()\n",
    "                \n",
    "                softmax = nn.Softmax(dim=-1)\n",
    "                final_prob = softmax(logits)\n",
    "                final = final_prob.argmax(dim=-1)\n",
    "                final[np.amax(np.array(final_prob.cpu()), axis=-1) < UNASSIGN_THRES] = -1\n",
    "                \n",
    "                predictions.append(final.cpu())\n",
    "                truths.append(labels_v.cpu())\n",
    "            \n",
    "            predictions = torch.cat(predictions)\n",
    "            truths = torch.cat(truths)\n",
    "            no_drop = predictions != -1\n",
    "            predictions = predictions[no_drop]\n",
    "            truths = truths[no_drop]\n",
    "            \n",
    "            cur_acc = accuracy_score(truths, predictions)\n",
    "            f1 = f1_score(truths, predictions, average='macro')\n",
    "            val_loss = running_loss / len(val_loader)\n",
    "\n",
    "            print(f'Epoch: {i} | Validation Loss: {val_loss:.6f} | F1 Score: {f1:.6f}')\n",
    "            print(confusion_matrix(truths, predictions))\n",
    "            print(classification_report(truths,predictions, target_names=label_names.tolist(), digits=4))\n",
    "            \n",
    "            if cur_acc > max_acc:\n",
    "                max_acc = cur_acc\n",
    "                trigger_times = 0\n",
    "                src.utils_BERT.save_best_ckpt(i, model, optimizer, scheduler, val_loss, model_name, ckpt_dir)\n",
    "            else:\n",
    "                trigger_times += 1\n",
    "                if trigger_times > PATIENCE:\n",
    "                    break\n",
    "    del predictions, truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt['epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "STED",
   "language": "python",
   "name": "sted"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
